ninja_required_version = 1.3
cxx = c++
nvcc = /usr/local/cuda-12.6/bin/nvcc

cflags = -DTORCH_EXTENSION_NAME=FlashAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/andrewhinh/Desktop/Projects/flash-attn/.venv/lib/python3.12/site-packages/torch/include -isystem /home/andrewhinh/Desktop/Projects/flash-attn/.venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/andrewhinh/Desktop/Projects/flash-attn/.venv/lib/python3.12/site-packages/torch/include/TH -isystem /home/andrewhinh/Desktop/Projects/flash-attn/.venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda-12.6/include -isystem /home/andrewhinh/.local/share/uv/python/cpython-3.12.5-linux-x86_64-gnu/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17
post_cflags = 
cuda_cflags = -DTORCH_EXTENSION_NAME=FlashAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/andrewhinh/Desktop/Projects/flash-attn/.venv/lib/python3.12/site-packages/torch/include -isystem /home/andrewhinh/Desktop/Projects/flash-attn/.venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/andrewhinh/Desktop/Projects/flash-attn/.venv/lib/python3.12/site-packages/torch/include/TH -isystem /home/andrewhinh/Desktop/Projects/flash-attn/.venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda-12.6/include -isystem /home/andrewhinh/.local/share/uv/python/cpython-3.12.5-linux-x86_64-gnu/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -use_fast_math -std=c++17
cuda_post_cflags = 
cuda_dlink_post_cflags = 
ldflags = -shared -L/home/andrewhinh/Desktop/Projects/flash-attn/.venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.6/lib64 -lcudart

rule compile
  command = $cxx -MMD -MF $out.d $cflags -c $in -o $out $post_cflags
  depfile = $out.d
  deps = gcc

rule cuda_compile
  depfile = $out.d
  deps = gcc
  command = $nvcc --generate-dependencies-with-compile --dependency-output $out.d $cuda_cflags -c $in -o $out $cuda_post_cflags



rule link
  command = $cxx $in $ldflags -o $out

build main.o: compile /home/andrewhinh/Desktop/Projects/flash-attn/dist/main.cpp
build cuda.cuda.o: cuda_compile /home/andrewhinh/Desktop/Projects/flash-attn/dist/cuda.cu



build FlashAttention.so: link main.o cuda.cuda.o

default FlashAttention.so
